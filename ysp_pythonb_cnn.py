# -*- coding: utf-8 -*-
"""YSP PythonB CNN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pGSRzTgUk-dppyUnxA8ZwYCoyou0RKOr
"""

#connecting the google drive to the colab file
from google.colab import drive
drive.mount('/content/drive')

#train and validation google drive paths
PATH = '/content/drive/MyDrive/Colab Notebooks/PythonProject/train/'
val_path = '/content/drive/MyDrive/Colab Notebooks/PythonProject/val/'

#used to select optimzer for the CNN(nearly always Adam)

from tensorflow.keras.optimizers import Adam, Adadelta, Adagrad, Adamax, Ftrl, Nadam, RMSprop, SGD
def get_optimizer(optimizer_name, learning_rate):

    print('Selected Optimizer', optimizer_name)
    switcher = {
        'Adadelta': Adadelta(learning_rate=learning_rate),
        'Adagrad': Adagrad(learning_rate=learning_rate),
        'Adam': Adam(learning_rate=learning_rate),
        'Adamax': Adamax(learning_rate=learning_rate),
        'FTRL': Ftrl(learning_rate=learning_rate),
        'NAdam': Nadam(learning_rate=learning_rate),
        'RMSprop': RMSprop(learning_rate=learning_rate),
        'Gradient Descent': SGD(learning_rate=learning_rate)
    }
    return switcher.get(optimizer_name, Adam(learning_rate=learning_rate))

#CNN Training Step

#imports
import matplotlib.pyplot as plt
import numpy as np
import os
import tensorflow as tf
from keras.applications.resnet import preprocess_input
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Dense,GlobalAveragePooling2D
from keras.models import Model
from tensorflow.keras import regularizers
from tensorflow.keras.preprocessing import image_dataset_from_directory
from keras.callbacks import EarlyStopping
from tensorflow import keras

#Hyperparameters
epochs = 50
base_learning_rate = 0.0001
optimizer = 'Adam'
BATCH_SIZE = 32
num_classes = 8
IMG_SIZE = (224, 224)

#transfers data from google drive to a usable form
train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
train_generator = train_datagen.flow_from_directory(PATH,
                                                target_size=IMG_SIZE,
                                                color_mode='rgb',
                                                batch_size=BATCH_SIZE,
                                                class_mode='categorical',
                                                shuffle=True)
validation_generator = train_datagen.flow_from_directory(val_path,
                                                target_size=IMG_SIZE,
                                                color_mode='rgb',
                                                batch_size=BATCH_SIZE,
                                                class_mode='categorical',
                                                shuffle=False)

#transfer learning based(takes initial steps of training from MobileNetV2)
base_model = tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet', alpha=0.35)
for layer in base_model.layers:
    layer.trainable=False
model = create_model(base_model,num_classes)
model.compile(optimizer = get_optimizer(optimizer_name=optimizer,learning_rate=base_learning_rate),loss='CategoricalCrossentropy',metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])
early_stopping_monitor = EarlyStopping(
    monitor='val_accuracy',
    min_delta=0,
    patience=30,
    verbose=0,
    mode='auto',
    baseline=None,
    restore_best_weights=True
)
step_size_train = train_generator.n//train_generator.batch_size
#saves model metrics/parameters
history_fine = model.fit(train_generator,
                        epochs=epochs,
                        callbacks=[early_stopping_monitor],
                        validation_data = validation_generator,
                        verbose=1)

#stores change in metrics over time
import numpy as np
prc = history_fine.history['val_precision_7']
rec = history_fine.history['val_recall_7']
acc = history_fine.history['val_accuracy']
loss = history_fine.history['val_loss']

#seperates the label from the image within the validation set

def seperate_labels(generator):
    x_validation = []
    y_validation = []
    num_seen = 0

    for x, labels in generator:
        x_validation.append(x)
        y_validation.append([argmax(label) for label in labels])
        num_seen += len(x)
        if num_seen == generator.n: break

    x_validation = np.concatenate(x_validation)
    y_validation = np.concatenate(y_validation)
    return x_validation, y_validation

# Calculate and display the confusion matrix
import matplotlib.pyplot as plt
from numpy.core.fromnumeric import argmax
from sklearn.metrics import ConfusionMatrixDisplay

x_validation, y_validation = seperate_labels(validation_generator)
y_pred = model.predict(x_validation, batch_size=BATCH_SIZE)
predictions = np.apply_along_axis(argmax, 1, y_pred)
display_labels = validation_generator.class_indices.keys()

ConfusionMatrixDisplay.from_predictions(y_validation, predictions, display_labels=display_labels, cmap="binary")
plt.show()

#Generate Accuracy Time Graph
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
plt.plot(np.arange(15), acc, color='green',
     linewidth=2, markersize=12)

plt.xlabel("Epoch Count")
plt.ylabel("Accuracy")

plt.show()

#Generate Precision-Recall Graph or F1 Graph
plt.plot(rec, prc, color='red',
     linewidth=2, markersize=12)
plt.xlabel("Recall")
plt.ylabel("Precision")

plt.show()